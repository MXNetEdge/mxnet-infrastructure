{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Bot Prediction Service #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Parser ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\n",
    "# This script serves to do data cleaning\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "# fix ssl certificate errors\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import re\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "\n",
    "class SentenceParser:\n",
    "\n",
    "    regex_str = [\n",
    "        r'<[^>]+>',                                                                     # HTML tags\n",
    "        r'(?:@[\\w_]+)',                                                                 # @-mentions\n",
    "        r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\",                                               # hash-tags\n",
    "        r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+',   # URLs\n",
    "        r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)',                                                   # numbers\n",
    "        r\"(?:[a-z][a-z'\\-_]+[a-z])\",                                                    # words with - and '\n",
    "        r'(?:[\\w_]+)',                                                                  # other words\n",
    "        r'(?:\\S)'                                                                       # anything else\n",
    "    ]\n",
    "    # English Stopwords\n",
    "    with open('stopwords.txt') as file:\n",
    "        stopwords = file.read().split()\n",
    "    file.close()\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        SentenceParser serves to clean text content\n",
    "        \"\"\"\n",
    "        self.data = None\n",
    "        # extract words stem\n",
    "        self.porter = nltk.PorterStemmer()\n",
    "        # a set of stopwords\n",
    "        self.stops = set(self.stopwords)\n",
    "\n",
    "    def read_file(self, filepath, filetype, encod='ISO-8859-1', header=None):\n",
    "        \"\"\"\n",
    "        This method is to read csv/json/xlsx files\n",
    "        \"\"\"\n",
    "        logging.info('Start reading File')\n",
    "        if not os.path.isfile(filepath):\n",
    "            logging.error(\"File Not Exist!\")\n",
    "            sys.exit()\n",
    "        if filetype == 'csv':\n",
    "            df = pd.read_csv(filepath, encoding=encod, header=header)\n",
    "        elif filetype == 'json':\n",
    "            df = pd.read_json(filepath, encoding=encod, lines=False)\n",
    "        elif filetype == 'xlsx':\n",
    "            df = pd.read_excel(filepath, encoding=encod, header=header)\n",
    "        else:\n",
    "            logging.error(\"Extension Type not Accepted!\")\n",
    "            sys.exit()\n",
    "\n",
    "        logging.debug(df)\n",
    "        self.data = df\n",
    "\n",
    "    def merge_column(self, columns, name):\n",
    "        \"\"\"\n",
    "        This method is to merge columns of a pandas dataframe\n",
    "        \"\"\"\n",
    "        logging.info('Merge headers %s to %s', str(columns), name)\n",
    "        self.data[name] = ''\n",
    "        for header in columns:\n",
    "            self.data[name] += ' ' + self.data[header]\n",
    "  \n",
    "    def clean_body(self, column, remove_template=True, remove_code=True):\n",
    "        \"\"\"\n",
    "        This methods is to remove template and code from issue's body\n",
    "        \"\"\"\n",
    "        logging.info(\"Start Removing Templates..\")\n",
    "        for i in range(len(self.data)):\n",
    "            # remove 'Environment info' part\n",
    "            if remove_template and \"## Environment info\" in self.data[column][i]:\n",
    "                index = self.data.loc[i, column].find(\"## Environment info\")\n",
    "                self.data.loc[i, column] = self.data.loc[i, column][:index]\n",
    "            # remove code\n",
    "            if remove_code and \"```\" in self.data[column][i]:\n",
    "                sample = self.data[column][i].split(\"```\")\n",
    "                sample = [sample[i*2] for i in range(0, int((len(sample)+1)/2))]\n",
    "                self.data.loc[i, column] = \" \".join(sample)\n",
    "\n",
    "    def process_text(self, column, remove_symbol=True, remove_stopwords=False, stemming=False):\n",
    "        \"\"\"\n",
    "        This method is to remove symbols/remove stopwords/extract words stem\n",
    "        \"\"\"\n",
    "        logging.info(\"Start Data Cleaning...\")\n",
    "        # remove some symbols\n",
    "        self.data[column] = self.data[column].str.replace(r'[\\n\\r\\t]+', ' ')\n",
    "        # remove URLs\n",
    "        self.data[column] = self.data[column].str.replace(self.regex_str[3], ' ')\n",
    "        tempcol = self.data[column].values.tolist()\n",
    "\n",
    "        for i in range(len(tempcol)):\n",
    "            row = BeautifulSoup(tempcol[i], 'html.parser').get_text().lower()\n",
    "            # remove symbols\n",
    "            if remove_symbol:\n",
    "                row = re.sub('[^a-zA-Z]', ' ', row)\n",
    "            words = row.split()\n",
    "            # remove stopwords\n",
    "            if remove_stopwords:\n",
    "                words = [w for w in words if w not in self.stops and not w.replace('.', '', 1).isdigit()]\n",
    "            # extract words stem\n",
    "            if stemming:\n",
    "                words = [self.porter.stem(w) for w in words] \n",
    "            row = ' '.join(words)\n",
    "            tempcol[i] = row.lower()\n",
    "        return tempcol\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFetcher ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\n",
    "# This scipt is served to fetch GitHub issues into a json file\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "\n",
    "class DataFetcher:\n",
    "\n",
    "    def __init__(self,\n",
    "                 github_user=os.environ.get(\"github_user\"),\n",
    "                 github_oauth_token=os.environ.get(\"github_oauth_token\"),\n",
    "                 repo=os.environ.get(\"repo\")):\n",
    "        \"\"\"\n",
    "        This DataFetcher serves to fetch issues data\n",
    "        Args:\n",
    "            github_user(str): the github id. ie: \"CathyZhang0822\"\n",
    "            github_oauth_token(str): the github oauth token, paired with github_user to realize authorization\n",
    "            repo(str): the repo name\n",
    "        \"\"\"\n",
    "        self.github_user = github_user\n",
    "        self.github_oauth_token = github_oauth_token\n",
    "        self.repo = repo\n",
    "        self.auth = (self.github_user, self.github_oauth_token)\n",
    "        self.json_data = None\n",
    "\n",
    "    def cleanstr(self, raw_string, sub_string):\n",
    "        \"\"\"\n",
    "        This method is to convert all non-alphanumeric charaters from \n",
    "        raw_string into substring\n",
    "        \"\"\"\n",
    "        clean = re.sub(\"[^0-9a-zA-Z]\", sub_string, raw_string)\n",
    "        return clean.lower()\n",
    "\n",
    "    def count_pages(self, state):\n",
    "        \"\"\"\n",
    "        This method is to count how many pages of issues/labels in total\n",
    "        state can be \"open\"/\"closed\"/\"all\"\n",
    "        \"\"\"\n",
    "        url = 'https://api.github.com/repos/%s/issues' % self.repo\n",
    "        response = requests.get(url, {'state': state},\n",
    "                                auth=self.auth)\n",
    "        assert response.status_code == 200, \"Authorization failed\"\n",
    "        if \"link\" not in response.headers:\n",
    "            return 1\n",
    "        return int(self.cleanstr(response.headers['link'], \" \").split()[-3])\n",
    "    \n",
    "    def fetch_issues(self, issue_nums):\n",
    "        \"\"\"\n",
    "        This method is to fetch issues data\n",
    "        issue_num: a list of issue ids\n",
    "        return issues' data in pandas dataframe format\n",
    "        \"\"\"\n",
    "        assert issue_nums != [], \"Empty Input!\"\n",
    "        logging.info(\"Reading issues:{}\".format(\", \".join([str(num) for num in issue_nums])))\n",
    "        data = []\n",
    "        for number in issue_nums:\n",
    "            url = 'https://api.github.com/repos/' + self.repo + '/issues/' + str(number)\n",
    "            response = requests.get(url, auth=self.auth)\n",
    "            item = response.json()\n",
    "            assert 'title' in item, \"{} issues doesn't exist!\".format(str(number))\n",
    "            data += [{'id': str(number), 'title': item['title'], 'body': item['body']}]\n",
    "        return pd.DataFrame(data)\n",
    "\n",
    "    def data2json(self, state, labels=None, other_labels=False):\n",
    "        \"\"\"\n",
    "        This method is to store issues' data into a json file, return json file's name\n",
    "        state can be either \"open\"/\"closed\"/\"all\"\n",
    "        labels is a list of target labels we are interested in\n",
    "        other_labels can be either \"True\"/\"False\"\n",
    "        \"\"\"\n",
    "        assert state in set(['all', 'open', 'closed']), \"Invalid State!\"\n",
    "        logging.info(\"Reading {} issues..\".format(state))\n",
    "        pages = self.count_pages(state)\n",
    "        data = []\n",
    "        for x in range(1, pages+1):\n",
    "            url = 'https://api.github.com/repos/' + self.repo + '/issues?page=' + str(x) \\\n",
    "                  + '&per_page=30'.format(repo=self.repo)\n",
    "            response = requests.get(url,\n",
    "                                    {'state': state,\n",
    "                                     'base': 'master',\n",
    "                                     'sort': 'created'},\n",
    "                                    auth=self.auth)\n",
    "            for item in response.json():\n",
    "                if \"pull_request\" in item:\n",
    "                    continue\n",
    "                if \"labels\" in item:\n",
    "                    issue_labels=list(set([item['labels'][i]['name'] for i in range(len(item['labels']))]))\n",
    "                else:\n",
    "                    continue\n",
    "                if labels is not None:\n",
    "                    # fetch issue which has at least one target label\n",
    "                    for label in labels:\n",
    "                        if label in issue_labels:\n",
    "                            if other_labels:\n",
    "                                # besides target labels, we still want other labels\n",
    "                                data += [{'id': item['number'],'title': item['title'], 'body': item['body'], 'labels': issue_labels}]\n",
    "                            else:\n",
    "                                # only record target labels\n",
    "                                if(label in set([\"Feature\", \"Call for Contribution\", \"Feature request\"])):\n",
    "                                    label = \"Feature\"\n",
    "                                data += [{'id': item['number'], 'title': item['title'], 'body': item['body'], 'labels': label}]\n",
    "                            # if have this break, then we only pick up the first target label\n",
    "                            break\n",
    "                else:\n",
    "                    # fetch all issues\n",
    "                    data += [{'id': item['number'], 'title': item['title'], 'body': item['body'], 'labels': issue_labels}]\n",
    "        self.json_data = data\n",
    "        s_labels = \"_\".join(labels) if labels is not None else \"all_labels\"\n",
    "        filename = \"{}_data.json_{}\".format(state, s_labels)\n",
    "        logging.info(\"Writing json file..\")\n",
    "        with open(filename, 'w') as write_file:\n",
    "            json.dump(data, write_file)\n",
    "        logging.info(\"{} json file is ready!\".format(filename))\n",
    "        return filename\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\n",
    "# This script is served to train Machine Learning models\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tempfile\n",
    "import pickle\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    # target labels that we are interested in\n",
    "    labels = [\"Performance\", \"Test\", \"Question\",\n",
    "               \"Feature request\", \"Call for contribution\",\n",
    "               \"Feature\", \"Example\", \"Doc\",\n",
    "               \"Installation\", \"Build\", \"Bug\"]\n",
    "\n",
    "    def __init__(self, \n",
    "                 tv=TfidfVectorizer(min_df=0.00009, ngram_range=(1, 3), max_features=10000), \n",
    "                 clf=SVC(gamma=0.5, C=100, probability=True),\n",
    "                 tmp_dir = tempfile.TemporaryDirectory()\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Trainer is to train issues using Machine Learning methods.\n",
    "        self.labels(list): a list of target labels\n",
    "        self.tv: TFIDF model (trigram, max_features = 10000)\n",
    "        self.clf: Classifier (SVC, kenerl = 'rbf')\n",
    "        self.tmp_tv_file: tempfile to store Vectorizer\n",
    "        self.tmp_clf_file: tempfile to store Classifier\n",
    "        self.tmp_labels_file: tempfile to store Labels\n",
    "        \"\"\"\n",
    "        self.tv = tv\n",
    "        self.clf = clf\n",
    "        self.tmp_dir = tmp_dir\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        This method is to train and save models.\n",
    "        It has 5 steps:\n",
    "        1. Fetch issues\n",
    "        2. Clean data\n",
    "        3. Word embedding\n",
    "        4. Train models\n",
    "        5. Save models\n",
    "        \"\"\"\n",
    "        logging.info(\"Start training issues of general labels\")\n",
    "        # Step1: Fetch issues with general labels\n",
    "        logging.info(\"Fetching Data..\")\n",
    "        DF = DataFetcher()\n",
    "        filename = DF.data2json('all', self.labels, False)\n",
    "        # Step2: Clean data\n",
    "        logging.info(\"Cleaning Data..\")\n",
    "        SP = SentenceParser()\n",
    "        SP.read_file(filename, 'json')\n",
    "        SP.clean_body('body', True, True)\n",
    "        SP.merge_column(['title', 'title', 'title', 'body'], 'train')\n",
    "        text = SP.process_text('train', True, False, True)\n",
    "        df = SP.data\n",
    "        # Step3: Word Embedding\n",
    "        logging.info(\"Word Embedding..\")\n",
    "        # tv = TfidfVectorizer(min_df=0.00009, ngram_range=(1, 3), max_features=10000)\n",
    "        tv = self.tv\n",
    "        X = tv.fit_transform(text).toarray()\n",
    "        # Labels\n",
    "        labels = SP.data['labels']\n",
    "        le = LabelEncoder()\n",
    "        Y = le.fit_transform(labels)\n",
    "        # Step4: Train Classifier\n",
    "        # SVC, kernel = 'rbf'\n",
    "        logging.info(\"Training Data..\")\n",
    "        # clf = SVC(gamma=0.5, C=100, probability=True)\n",
    "        clf = self.clf\n",
    "        clf.fit(X, Y)\n",
    "        # Step5: save models\n",
    "        logging.info(\"Saving Models..\")\n",
    "        with open(os.path.join(self.tmp_dir.name,'Vectorizer.p'), 'wb') as tv_file:\n",
    "            pickle.dump(tv, tv_file)\n",
    "        with open(os.path.join(self.tmp_dir.name,'Classifier.p'), 'wb') as clf_file:\n",
    "            pickle.dump(clf, clf_file)\n",
    "        with open(os.path.join(self.tmp_dir.name,'Labels.p'), 'wb') as labels_file:\n",
    "            pickle.dump(labels, labels_file)\n",
    "        logging.info(\"Completed!\")\n",
    "        return self.tmp_dir\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed to the Apache Software Foundation (ASF) under one\n",
    "# or more contributor license agreements.  See the NOTICE file\n",
    "# distributed with this work for additional information\n",
    "# regarding copyright ownership.  The ASF licenses this file\n",
    "# to you under the Apache License, Version 2.0 (the\n",
    "# \"License\"); you may not use this file except in compliance\n",
    "# with the License.  You may obtain a copy of the License at\n",
    "#\n",
    "#   http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing,\n",
    "# software distributed under the License is distributed on an\n",
    "# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n",
    "# KIND, either express or implied.  See the License for the\n",
    "# specific language governing permissions and limitations\n",
    "# under the License.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import logging\n",
    "import os\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    # keywords will be used to apply rule-based algorithms\n",
    "    keywords = {\"ci\": [\"ci\", \"ccache\", \"jenkins\"],\n",
    "                \"flaky\": [\"flaky\"],\n",
    "                \"gluon\": [\"gluon\"],\n",
    "                \"coda\": [\"cuda\", \"cudnn\"],\n",
    "                \"scala\": [\"scala\"],\n",
    "                \"mkldnn\": [\"mkldnn, mkl\"],\n",
    "                \"onnx\": [\"onnx\"]}\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Predictor serves to apply rule-based and ML algorithms to predict labels\n",
    "        \"\"\"\n",
    "        self.tv = None\n",
    "        self.labels = None\n",
    "        self.clf = None\n",
    "\n",
    "    def reload(self, tmp_dir):\n",
    "        \"\"\"\n",
    "        This method is to load models\n",
    "        \"\"\"\n",
    "        with open(os.path.join(tmp_dir.name,'Vectorizer.p'), \"rb\") as tv:\n",
    "            self.tv = pickle.load(tv)\n",
    "        with open(os.path.join(tmp_dir.name,'Classifier.p'), \"rb\") as clf:\n",
    "            self.clf = pickle.load(clf)\n",
    "        with open(os.path.join(tmp_dir.name,'Labels.p'), \"rb\") as labels:\n",
    "            self.labels = pickle.load(labels)\n",
    "\n",
    "    def tokenize(self, row):\n",
    "        \"\"\"\n",
    "        This method is to tokenize a sentence into a list of words\n",
    "        Args:\n",
    "            row(string): a sentence\n",
    "        Return:\n",
    "            words(list): a list of words\n",
    "        \"\"\"\n",
    "        row = re.sub('[^a-zA-Z0-9]', ' ', row).lower()\n",
    "        words = set(row.split())\n",
    "        return words\n",
    "\n",
    "    def rule_based(self, issues):\n",
    "        \"\"\"\n",
    "        This method applies rule_based algorithms to predict labels\n",
    "        Args:\n",
    "            issues(list): a list of issue numbers\n",
    "        Return:\n",
    "            rule_based_predictions(list of lists): labels which satisfy rules\n",
    "        \"\"\"\n",
    "        DF = DataFetcher()\n",
    "        df_test = DF.fetch_issues(issues)\n",
    "        rule_based_predictions = []\n",
    "        for i in range(len(issues)):\n",
    "            # extract every issue's title\n",
    "            row = df_test.loc[i, 'title']\n",
    "            # apply rule-based algorithms\n",
    "            single_issue_predictions = []\n",
    "            if \"feature request\" in row.lower():\n",
    "                single_issue_predictions.append(\"Feature\")\n",
    "            if \"c++\" in row.lower():\n",
    "                single_issue_predictions.append(\"C++\")\n",
    "            tokens = self.tokenize(row)\n",
    "            for k, v in self.keywords.items():\n",
    "                for keyword in v:\n",
    "                    if keyword in tokens:\n",
    "                        single_issue_predictions.append(k)\n",
    "            rule_based_predictions.append(single_issue_predictions)\n",
    "        return rule_based_predictions\n",
    "\n",
    "    def ml_predict(self, issues, threshold=0.3):\n",
    "        \"\"\"\n",
    "        This method applies machine learning algorithms to predict labels\n",
    "        Args:\n",
    "            issues(list): a list of issue numbers\n",
    "            threshold(float): threshold of probability\n",
    "        Return:\n",
    "            ml_predictions(list of lists): predictions\n",
    "        \"\"\"\n",
    "        # step1: fetch data\n",
    "        DF = DataFetcher()\n",
    "        df_test = DF.fetch_issues(issues)\n",
    "        # step2: data cleaning\n",
    "        SP = SentenceParser()\n",
    "        SP.data = df_test\n",
    "        SP.clean_body('body', True, True)\n",
    "        SP.merge_column(['title', 'title', 'title', 'body'], 'train')\n",
    "        test_text = SP.process_text('train', True, False, True)\n",
    "        # step3: word embedding\n",
    "        test_data_tfidf = self.tv.transform(test_text).toarray()\n",
    "        le = LabelEncoder()\n",
    "        le.fit_transform(self.labels)\n",
    "        # step4: classification\n",
    "        probs = self.clf.predict_proba(test_data_tfidf)\n",
    "        # pick up top 2 predictions which exceeds threshold\n",
    "        best_n = np.argsort(probs, axis=1)[:, -2:]\n",
    "        ml_predictions = []\n",
    "        for i in range(len(best_n)):\n",
    "            # INFO:Predictor:issue:11919,Performance:0.47353076240017744,Question:0.2440056213336274\n",
    "            logging.info(\"issue:{}, {}:{}, {}:{}\".format(str(issues[i]), str(le.classes_[best_n[i][-1]]), str(probs[i][best_n[i][-1]]),\n",
    "                        str(le.classes_[best_n[i][-2]]), str(probs[i][best_n[i][-2]])))\n",
    "            single_issue_predictions = [le.classes_[best_n[i][j]] for j in range(-1, -3, -1) if probs[i][best_n[i][j]] > threshold]\n",
    "            ml_predictions.append(single_issue_predictions)\n",
    "        return ml_predictions\n",
    "\n",
    "    def predict(self, issues):\n",
    "        # return predictions of both rule_base algorithms and machine learning methods\n",
    "        rule_based_predictions = self.rule_based(issues)\n",
    "        ml_predictions = self.ml_predict(issues)\n",
    "        predictions = [list(set(rule_based_predictions[i]+ml_predictions[i])) for i in range(len(ml_predictions))]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
